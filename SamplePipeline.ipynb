{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "subtle-killer",
   "metadata": {},
   "source": [
    "## Azure ML Pipeline - Parallel Processing for Fuzzy Matching\n",
    "This notebook demonstrates creation & execution of an Azure ML pipeline designed to read data from an attached Azure Blob Datastore (using dynamic arguments passed as a pipeline parameter), process that data, and then export the result dataset back to blob storage in both Excel and CSV format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developed-recording",
   "metadata": {},
   "source": [
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-single",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, Datastore, Environment, Dataset\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute, DataFactoryCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.pipeline.core import Pipeline, PipelineParameter, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import PipelineParameter, PipelineData\n",
    "from azureml.data.output_dataset_config import OutputTabularDatasetConfig, OutputDatasetConfig, OutputFileDatasetConfig\n",
    "from azureml.data.datapath import DataPath\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.data.sql_data_reference import SqlDataReference\n",
    "from azureml.pipeline.steps import DataTransferStep\n",
    "from azureml.pipeline.steps import ParallelRunConfig, ParallelRunStep\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-escape",
   "metadata": {},
   "source": [
    "### Connect to Azure ML Workspace, Provision Compute Resources, and get References to Datastore\n",
    "\n",
    "Connect to workspace using config associated config file. Get a reference to you pre-existing AML compute cluster or provision a new cluster to facilitate processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-rating",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "#Select AML Compute Cluster\n",
    "cpu_cluster_name = 'cpucluster'\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found an existing cluster, using it instead.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D13_V2',\n",
    "                                                           min_nodes=0,\n",
    "                                                           max_nodes=10)\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "    cpu_cluster.wait_for_completion(show_output=True)\n",
    "    \n",
    "\n",
    "#Get Default Datastore\n",
    "default_ds = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-gothic",
   "metadata": {},
   "source": [
    "### Create Run Configuration\n",
    "The `RunConfiguration` defines the environment used across all python steps. You can optionally add additional conda or pip packages to be added to your environment. [More details here](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.conda_dependencies.condadependencies?view=azure-ml-py).\n",
    "~~~~\n",
    "run_config.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=['requests'])\n",
    "run_config.environment.python.conda_dependencies.add_pip_package('azureml-opendatasets')\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-detector",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = RunConfiguration()\n",
    "run_config.environment.docker.enabled = True\n",
    "run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\n",
    "run_config.environment.python.conda_dependencies.add_pip_package('openpyxl==3.0.7')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-rendering",
   "metadata": {},
   "source": [
    "### Define Output Datasets\n",
    "Below we define the configuration for datasets that will be passed between steps in our pipeline. Note, in all cases we specify the datastore that should hold the datasets and whether they should be registered following step completion or not. This can optionally be disabled by removing the `register_on_complete()` call. We also leverage a `PipelineData` object as an intermediary before saving data to well-formatted CSV/Excel files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuck-gambling",
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_dataset = OutputFileDatasetConfig(name='excel_data', destination=(default_ds, 'excel_data/{run-id}')).register_on_complete(name='excel_data')\n",
    "sql_dataset = OutputFileDatasetConfig(name='sql_data', destination=(default_ds, 'sql_data/{run-id}')).read_delimited_files().register_on_complete(name='sql_data')\n",
    "processed_dataset_tabular = OutputFileDatasetConfig(name='processed_data_tabular', destination=(default_ds, 'processed_data_tabular/{run-id}')).read_delimited_files().register_on_complete(name='processed_data_tabular')\n",
    "processed_dataset_file = OutputFileDatasetConfig(name='processed_data_file', destination=(default_ds, 'processed_data_file/{run-id}')).register_on_complete(name='processed_data_file')\n",
    "processed_dataset_pipeline_data = PipelineData(name='processed_data', datastore=default_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerical-dubai",
   "metadata": {},
   "source": [
    "### Define Pipeline Parameters\n",
    "`PipelineParameter` objects serve as variable inputs to an Azure ML pipeline and can be specified at runtime. Below we specify a two pipeline parameter objects `excel_path_param` and `sql_path_param` which will be used to define the locations of target data inside the default Azure ML Blob datastore, respectively. Multiple pipeline parameters can be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-carolina",
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_path_param = PipelineParameter(name='excel_path_parameter', default_value='lalanding/<SAMPLE_RUN_ID>/')\n",
    "sql_path_param = PipelineParameter(name='sql_path_parameter', default_value='adflanding/<SAMPLE_RUN_ID>')\n",
    "min_year_param = PipelineParameter(name='min_year', default_value=2019)\n",
    "max_year_param = PipelineParameter(name='max_year', default_value=2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-wednesday",
   "metadata": {},
   "source": [
    "### Define Pipeline Steps\n",
    "The pipeline below consists of four steps - two steps to gather and register Excel/SQL data, a processing step where fuzz matching code should go, and a data organization/export step. All of the `PythonScriptStep`s have a corresponding `*.py` file which is referenced in the step arguments. Also, any `PipelineParameter`s defined above can be passed to and consumed within these steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-armstrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Excel file specified by the excel_path_param\n",
    "#Read individual sheets of data and save as separate CSVs in the excel_dataset location\n",
    "#Register data upon completion\n",
    "register_excel_data_step = PythonScriptStep(\n",
    "    name='register-excel-data',\n",
    "    script_name='register_excel_data.py',\n",
    "    arguments =['--excel_path_param', excel_path_param,\n",
    "               '--excel_dataset', excel_dataset],\n",
    "    outputs=[excel_dataset],\n",
    "    compute_target=cpu_cluster,\n",
    "    source_directory='.',\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")\n",
    "\n",
    "#Get CSV file containing queried SQL data\n",
    "#Register tabular dataset after retrieval from Blob Storage\n",
    "register_sql_data_step = PythonScriptStep(\n",
    "    name='register-sql-data',\n",
    "    script_name='register_sql_data.py',\n",
    "    arguments =['--sql_path_param', sql_path_param,\n",
    "               '--sql_dataset', sql_dataset],\n",
    "    outputs=[sql_dataset],\n",
    "    compute_target=cpu_cluster,\n",
    "    source_directory='.',\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")\n",
    "\n",
    "\n",
    "#Process Data\n",
    "#Parallel step to perform fuzzy matching\n",
    "#All results are appended as individual rows to a table that will be converted to a Pandas\n",
    "#dataframe and exported to CSV/Excel in the final step (organize_results_step)\n",
    "#Results are captured in a PipelineData object (processed_dataset_pipeline_data) passed to the final step\n",
    "\n",
    "#The settings below define the number of processes that will run per node, \n",
    "#and the number of nodes available for processing.\n",
    "#Ting - adjust these settings upwards to reduce processing time\n",
    "processes_per_node = 8\n",
    "node_count = 1\n",
    "timeout = 180\n",
    "\n",
    "parallel_run_config = ParallelRunConfig(\n",
    "    source_directory='.',\n",
    "    entry_script='process_data.py',\n",
    "    mini_batch_size=\"1\",\n",
    "    run_invocation_timeout=timeout,\n",
    "    error_threshold=10,\n",
    "    output_action=\"append_row\",\n",
    "    environment=run_config.environment,\n",
    "    process_count_per_node=processes_per_node,\n",
    "    compute_target=cpu_cluster,\n",
    "    node_count=node_count)\n",
    "\n",
    "parallel_fuzzy_matching_step = ParallelRunStep(\n",
    "    name=\"parallel-fuzzy-matching-step\",\n",
    "    parallel_run_config=parallel_run_config,\n",
    "    inputs=[excel_dataset.as_input(name='excel_data')],\n",
    "    side_inputs=[sql_dataset.as_input(name='sql_data')],\n",
    "    output=processed_dataset_pipeline_data,\n",
    "    allow_reuse=False\n",
    ")\n",
    "\n",
    "#Final step where fuzzy matching results are organized into a well-formatted dataframe\n",
    "#and exported both to CSV (registered as a Tabular dataset in the AML workspace)\n",
    "#and saved to Excel (to be consumed by Ting/internal clients)\n",
    "organize_results_step = PythonScriptStep(\n",
    "    name='organize_results_step',\n",
    "    script_name='organize_results.py',\n",
    "    arguments =['--processed_dataset_tabular', processed_dataset_tabular,\n",
    "               '--processed_dataset_file', processed_dataset_file,\n",
    "               '--processed_dataset', processed_dataset_pipeline_data],\n",
    "    inputs=[processed_dataset_pipeline_data],\n",
    "    outputs=[processed_dataset_tabular, processed_dataset_file],\n",
    "    compute_target=cpu_cluster,\n",
    "    source_directory='.',\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fiscal-brief",
   "metadata": {},
   "source": [
    "### Create Pipeline\n",
    "Create an Azure ML Pipeline by specifying the steps to be executed. Note: based on the dataset dependencies between steps, exection occurs logically such that no step will execute unless all of the necessary input datasets have been generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-referral",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[register_excel_data_step, register_sql_data_step, parallel_fuzzy_matching_step, organize_results_step])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-cookbook",
   "metadata": {},
   "source": [
    "### Create Experiment and Run Pipeline\n",
    "Define a new experiment (logical container for pipeline runs) and execute the pipeline. You can modify the values of pipeline parameters here when submitting a new run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-fighter",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(ws, 'pipeline-development')\n",
    "run = experiment.submit(pipeline)\n",
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-sunglasses",
   "metadata": {},
   "source": [
    "### Publish Pipeline\n",
    "Create a published version of your pipeline that can be triggered via a REST API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-serbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline.publish(name = 'fuzzy_matching_aml_pipeline',\n",
    "                                     description = 'Sample pipeline that registers excel/sql datasets',\n",
    "                                     continue_on_step_failure = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}